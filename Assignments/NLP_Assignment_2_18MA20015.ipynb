{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyJ25uz0kSaw"
   },
   "source": [
    "# Assignment 2 on Natural Language Processing\n",
    "\n",
    "### Date : 15th Sept, 2020\n",
    "\n",
    "### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ao1nhg9RknmF"
   },
   "source": [
    "The central idea of this assignment is to make you familiar with programming in python and also the language modelling task of natural language processing using the python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stk58juYkzEr"
   },
   "source": [
    "**Dataset:** \n",
    "\n",
    " Use the text file provided along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rT6byv49kdmo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a', 'H', '“', '!', 'u', 'i', 'o', 'y', 'P', '\\n', 'w', ',', ')', 'D', 'A', 'g', 'R', 'x', 'z', 'Z', 'N', 'b', 'L', 'n', 'l', 'f', '—', ';', 'I', 'M', 'U', 'd', '(', ' ', 'ù', 'c', 'h', 'p', 't', 'm', 'j', 'v', 'T', 'V', 'F', 'W', 'k', ']', 'O', 'e', '’', 'J', '_', '.', 'K', 'E', 's', '”', 'q', 'r', 'G', 'Q', ':', 'Y', '[', 'S', '-', 'C', 'B', 'X', '‘', '?'}\n"
     ]
    }
   ],
   "source": [
    "# reading the dataset\n",
    "f=open('corpus.txt',encoding=\"utf8\")\n",
    "s=f.read()\n",
    "f.close()\n",
    "\n",
    "# looking what all chars are there in the corpus\n",
    "print(set(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the requirements\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "\n",
    "# loding the english stopwords\n",
    "eng_stopwords=stopwords.words('english')\n",
    "\n",
    "# utility function to replace/remove chars from the string\n",
    "def remove_chars(s,charlist,replaceby=' '):\n",
    "    sp=s\n",
    "    for c in charlist:\n",
    "        sp=sp.replace(c,replaceby)\n",
    "    return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important chars to remove\n",
    "# note we are not removing '-' before tokenization as '-' does chnage the meaning of words\n",
    "charlist=['\\n',']','[',')','(','_']\n",
    "chars_to_remove=[',','”','’','?','“',';','‘',':','—','_','!','-','.']\n",
    "SENTENCE_END='.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRGqKaDn1pJy"
   },
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C1OtHn6B1oc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2878"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing the newline and other  brackets\n",
    "corpus=remove_chars(s,charlist)\n",
    "\n",
    "# Converting the corpus to lower case to remove the case sensitivity, can be removed if case sensitivity is desired\n",
    "corpus=corpus.lower()\n",
    "\n",
    "# tokenize the corpus\n",
    "corpus_sents=sent_tokenize(corpus)\n",
    "corpus_toks=[w for w in word_tokenize(corpus) if w not in chars_to_remove]\n",
    "\n",
    "# all words in the courpus\n",
    "Vocab=list(set(corpus_toks))\n",
    "Voc_size=len(Vocab)\n",
    "Voc_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YDL7yfpXkMRS"
   },
   "source": [
    "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
    "\n",
    "1. **Create the following language models** on the training corpus: <br>\n",
    "    i.   Unigram <br>\n",
    "    ii.  Bigram <br>\n",
    "    iii. Trigram <br>\n",
    "    iv.  Fourgram <br>\n",
    "\n",
    "2. **List the top 5 bigrams, trigrams, four-grams (with and without Add-1 smoothing).**\n",
    "(Note: Please remove those which contain only articles, prepositions, determiners. For Example: “of the”, “in a”, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helpful functions\n",
    "\n",
    "def get_freqdist(corpus_sent,n,smoothing=False,remove_stop_words=False):\n",
    "    \"\"\"\n",
    "    function to return the frequency dict of n-gram with or without smoothing and with or without removing stopwords\n",
    "    \"\"\"\n",
    "    eng_stopwords=stopwords.words('english')\n",
    "    chars_to_remove=[',','”','’','?','“',';','‘',':','—','_','!','-','.']\n",
    "    \n",
    "    # adding the n-grams sentence by sentence\n",
    "    grams=[]\n",
    "    for sent in corpus_sent:\n",
    "        content=[w for w in word_tokenize(sent) if w not in chars_to_remove]\n",
    "        grams.extend(ngrams(content,n))\n",
    "        \n",
    "    l=[x for x in grams]\n",
    "    \n",
    "    # removing the n-grams which only contain stopwords\n",
    "    if remove_stop_words:\n",
    "        ll=[]\n",
    "        for x in l:\n",
    "            is_all_stopwords=True\n",
    "            for tok in list(x):\n",
    "                if tok not in eng_stopwords:\n",
    "                    is_all_stopwords=False\n",
    "                    break\n",
    "            if not is_all_stopwords:\n",
    "                ll.append(x)\n",
    "        l=ll\n",
    "    \n",
    "    # calculate the frequency of n-grams\n",
    "    dist = nltk.FreqDist(l)\n",
    "    \n",
    "    # counts for add-1 smoothing, just need to add to the ones which are already there\n",
    "    # for others which are not in the keys they as just 1, this saves space by not storing the Voc_Size**n combinations\n",
    "    if smoothing:\n",
    "        for key in dist.keys():\n",
    "            dist[key]+=1\n",
    "\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 common unigrams without removing stopwords and without smoothing:\n",
      "[(('the',), 1630), (('and',), 843), (('to',), 719), (('a',), 626), (('it',), 579), (('she',), 550), (('i',), 532), (('of',), 508), (('said',), 459), (('you',), 404)]\n",
      "\n",
      "\n",
      "Top 10 common bigrams without removing stopwords and without smoothing:\n",
      "[(('said', 'the'), 208), (('of', 'the'), 130), (('said', 'alice'), 116), (('in', 'a'), 97), (('and', 'the'), 80), (('in', 'the'), 78), (('it', 'was'), 73), (('the', 'queen'), 72), (('to', 'the'), 69), (('the', 'king'), 62)]\n",
      "\n",
      "\n",
      "Top 10 common trigrams without removing stopwords and without smoothing:\n",
      "[(('the', 'mock', 'turtle'), 53), (('i', 'don', 't'), 31), (('the', 'march', 'hare'), 30), (('said', 'the', 'king'), 29), (('the', 'white', 'rabbit'), 21), (('said', 'the', 'hatter'), 20), (('said', 'to', 'herself'), 19), (('said', 'the', 'mock'), 19), (('said', 'the', 'caterpillar'), 18), (('she', 'went', 'on'), 17)]\n",
      "\n",
      "\n",
      "Top 10 common fourgrams without removing stopwords and without smoothing:\n",
      "[(('said', 'the', 'mock', 'turtle'), 19), (('she', 'said', 'to', 'herself'), 16), (('a', 'minute', 'or', 'two'), 11), (('you', 'won', 't', 'you'), 10), (('said', 'the', 'march', 'hare'), 8), (('will', 'you', 'won', 't'), 8), (('said', 'alice', 'in', 'a'), 7), (('i', 'don', 't', 'know'), 7), (('as', 'well', 'as', 'she'), 6), (('well', 'as', 'she', 'could'), 6)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## without removing the stopwords\n",
    "top=10\n",
    "\n",
    "unidist = get_freqdist(corpus_sents,1,False,False)\n",
    "print(\"Top {} common unigrams without removing stopwords and without smoothing:\".format(top),\n",
    "      unidist.most_common(top),'\\n',sep='\\n')\n",
    "\n",
    "bidist = get_freqdist(corpus_sents,2,False,False)\n",
    "print(\"Top {} common bigrams without removing stopwords and without smoothing:\".format(top),\n",
    "      bidist.most_common(top),'\\n',sep='\\n')\n",
    "\n",
    "tridist = get_freqdist(corpus_sents,3,False,False)\n",
    "print(\"Top {} common trigrams without removing stopwords and without smoothing:\".format(top),\n",
    "      tridist.most_common(top),'\\n',sep='\\n')\n",
    "\n",
    "fourdist = get_freqdist(corpus_sents,4,False,False)\n",
    "print(\"Top {} common fourgrams without removing stopwords and without smoothing:\".format(top),\n",
    "      fourdist.most_common(top),'\\n',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vARsvSfynePr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 common unigrams with removing stopwords and without smoothing:\n",
      "[(('said',), 459), (('alice',), 397), (('little',), 128), (('one',), 102), (('like',), 84), (('would',), 83), (('went',), 83), (('could',), 77), (('queen',), 75), (('thought',), 74)]\n",
      "\n",
      "\n",
      "Top 10 common bigrams with removing stopwords and without smoothing:\n",
      "[(('said', 'the'), 208), (('said', 'alice'), 116), (('the', 'queen'), 72), (('the', 'king'), 62), (('a', 'little'), 59), (('mock', 'turtle'), 56), (('the', 'mock'), 53), (('the', 'gryphon'), 53), (('the', 'hatter'), 52), (('went', 'on'), 48)]\n",
      "\n",
      "\n",
      "Top 10 common trigrams with removing stopwords and without smoothing:\n",
      "[(('the', 'mock', 'turtle'), 53), (('the', 'march', 'hare'), 30), (('said', 'the', 'king'), 29), (('the', 'white', 'rabbit'), 21), (('said', 'the', 'hatter'), 20), (('said', 'to', 'herself'), 19), (('said', 'the', 'mock'), 19), (('said', 'the', 'caterpillar'), 18), (('she', 'went', 'on'), 17), (('she', 'said', 'to'), 17)]\n",
      "\n",
      "\n",
      "Top 10 common fourgrams with removing stopwords and without smoothing:\n",
      "[(('said', 'the', 'mock', 'turtle'), 19), (('she', 'said', 'to', 'herself'), 16), (('a', 'minute', 'or', 'two'), 11), (('said', 'the', 'march', 'hare'), 8), (('said', 'alice', 'in', 'a'), 7), (('i', 'don', 't', 'know'), 7), (('as', 'well', 'as', 'she'), 6), (('well', 'as', 'she', 'could'), 6), (('in', 'a', 'great', 'hurry'), 6), (('in', 'a', 'tone', 'of'), 6)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print top 10 unigrams, bigrams after removing ngrams which only contain stopwords\n",
    "top=10\n",
    "\n",
    "unidist = get_freqdist(corpus_sents,1,False,True)\n",
    "print(\"Top {} common unigrams with removing stopwords and without smoothing:\".format(top),\n",
    "      unidist.most_common(top),'\\n',sep='\\n')\n",
    "\n",
    "bidist = get_freqdist(corpus_sents,2,False,True)\n",
    "print(\"Top {} common bigrams with removing stopwords and without smoothing:\".format(top),\n",
    "      bidist.most_common(top),'\\n',sep='\\n')\n",
    "\n",
    "tridist = get_freqdist(corpus_sents,3,False,True)\n",
    "print(\"Top {} common trigrams with removing stopwords and without smoothing:\".format(top),\n",
    "      tridist.most_common(top),'\\n',sep='\\n')\n",
    "\n",
    "fourdist = get_freqdist(corpus_sents,4,False,True)\n",
    "print(\"Top {} common fourgrams with removing stopwords and without smoothing:\".format(top),\n",
    "      fourdist.most_common(top),'\\n',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ioc1xNjmnim-"
   },
   "source": [
    "# Applying Smoothing\n",
    "\n",
    "\n",
    "Assume additional training data in which each possible N-gram occurs exactly once and adjust estimates.\n",
    "\n",
    "> ### $ Probability(ngram) = \\frac{Count(ngram)+1}{ N\\, +\\, V} $\n",
    "\n",
    "N: Total number of N-grams <br>\n",
    "V: Number of unique N-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grh4sO0Yns4V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 common unigrams with removing stopwords and with smoothing:\n",
      "[(('said',), 460), (('alice',), 398), (('little',), 129), (('one',), 103), (('like',), 85), (('would',), 84), (('went',), 84), (('could',), 78), (('queen',), 76), (('thought',), 75)]\n",
      "\n",
      "\n",
      "Top 10 common bigrams with removing stopwords and with smoothing:\n",
      "[(('said', 'the'), 209), (('said', 'alice'), 117), (('the', 'queen'), 73), (('the', 'king'), 63), (('a', 'little'), 60), (('mock', 'turtle'), 57), (('the', 'mock'), 54), (('the', 'gryphon'), 54), (('the', 'hatter'), 53), (('went', 'on'), 49)]\n",
      "\n",
      "\n",
      "Top 10 common trigrams with removing stopwords and with smoothing:\n",
      "[(('the', 'mock', 'turtle'), 54), (('the', 'march', 'hare'), 31), (('said', 'the', 'king'), 30), (('the', 'white', 'rabbit'), 22), (('said', 'the', 'hatter'), 21), (('said', 'to', 'herself'), 20), (('said', 'the', 'mock'), 20), (('said', 'the', 'caterpillar'), 19), (('she', 'went', 'on'), 18), (('she', 'said', 'to'), 18)]\n",
      "\n",
      "\n",
      "Top 10 common fourgrams with removing stopwords and with smoothing:\n",
      "[(('said', 'the', 'mock', 'turtle'), 20), (('she', 'said', 'to', 'herself'), 17), (('a', 'minute', 'or', 'two'), 12), (('said', 'the', 'march', 'hare'), 9), (('said', 'alice', 'in', 'a'), 8), (('i', 'don', 't', 'know'), 8), (('as', 'well', 'as', 'she'), 7), (('well', 'as', 'she', 'could'), 7), (('in', 'a', 'great', 'hurry'), 7), (('in', 'a', 'tone', 'of'), 7)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#You are to perform Add-1 smoothing here:\n",
    "top=10\n",
    "\n",
    "unidist = get_freqdist(corpus_sents,1,True,True)\n",
    "print(\"Top {} common unigrams with removing stopwords and with smoothing:\".format(top),\n",
    "      unidist.most_common(top),'\\n',sep='\\n')\n",
    "\n",
    "bidist = get_freqdist(corpus_sents,2,True,True)\n",
    "print(\"Top {} common bigrams with removing stopwords and with smoothing:\".format(top),\n",
    "      bidist.most_common(top),'\\n',sep='\\n')\n",
    "\n",
    "tridist = get_freqdist(corpus_sents,3,True,True)\n",
    "print(\"Top {} common trigrams with removing stopwords and with smoothing:\".format(top),\n",
    "      tridist.most_common(top),'\\n',sep='\\n')\n",
    "\n",
    "fourdist = get_freqdist(corpus_sents,4,True,True)\n",
    "print(\"Top {} common fourgrams with removing stopwords and with smoothing:\".format(top),\n",
    "      fourdist.most_common(top),'\\n',sep='\\n')\n",
    "\n",
    "#Print top 10 unigram, bigram, trigram, fourgram after smoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0GL40mQmmt4"
   },
   "source": [
    "### Predict the next word using statistical language modelling\n",
    "\n",
    "Using the above bigram, trigram, and fourgram models that you just experimented with, **predict the next word(top 5 probable) given the previous n(=2, 3, 4)-grams** for the sentences below.\n",
    "\n",
    "For str1, str2, you are to predict the next  2 possible word sequences using your trained smoothed models. <br> \n",
    "For example, for the string 'He looked very' the answers can be as below: \n",
    ">     (1) 'He looked very' *anxiouxly* \n",
    ">     (2) 'He looked very' *uncomfortable* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to get log-probability of n-grams\n",
    "class Probs:\n",
    "    def __init__(self,freqdict,n,V):\n",
    "        self.n=n\n",
    "        self.dict=freqdict\n",
    "        self.V=V\n",
    "        self.N=sum(freqdict.values())\n",
    "        \n",
    "    def logprob(self,key):\n",
    "        # find if key there and if there return the log-prob else return \n",
    "        if key in self.dict.keys():\n",
    "            return np.log(self.dict[key])-np.log(self.N)\n",
    "\n",
    "        # return the default value lower than any other value\n",
    "        # this can be removed and a error statement can be added\n",
    "        return -np.log(2*self.N)\n",
    "        \n",
    "    def slogprob(self,key):\n",
    "        # find if key there and if there return the smoothed-log-prob else return -log(V+N) as that is the smooth-log-prob\n",
    "        if key in self.dict.keys():\n",
    "            return np.log(self.dict[key]+1)-np.log(self.N+self.V**self.n)\n",
    "        return -np.log(self.N+self.V**self.n)\n",
    "    \n",
    "    def get_best_s(self,count=10):\n",
    "        k=self.dict.keys()\n",
    "        # sorting the n-grams according to the smoothed-log-probs and returns the top count\n",
    "        return sorted(k,key=self.slogprob,reverse=True)[0:count]\n",
    "    \n",
    "    def get_best(self,count=10):\n",
    "        k=self.dict.keys()\n",
    "        # sorting the n-grams according to the log-probs and returns the top count\n",
    "        return sorted(k,key=self.logprob,reverse=True)[0:count]\n",
    "    \n",
    "\n",
    "# class to predict the next word from the log-probability of n-grams\n",
    "class Model:\n",
    "    def __init__(self,freqdict_n,lag_freqdict_n,n,V,Vocab):\n",
    "        self.n=n\n",
    "        self.probn=Probs(freqdict_n,n,V)\n",
    "        self.lag_probn=Probs(lag_freqdict_n,n-1,V)\n",
    "        self.V=V\n",
    "        self.Vocab=Vocab\n",
    "        self.top=5\n",
    "        \n",
    "    def predict(self,k):\n",
    "        # array to store log probs\n",
    "        prob=np.full(self.V,0)\n",
    "        i=0\n",
    "        # we go through each word and get its smoothed-log-prob in the prob array\n",
    "        for w in self.Vocab:\n",
    "            y=self.lag_probn.slogprob(k)\n",
    "            kc=list(k)\n",
    "            kc.append(w)\n",
    "            x=self.probn.slogprob(tuple(kc))-self.lag_probn.slogprob(k)\n",
    "            prob[i]=x\n",
    "            i+=1\n",
    "            \n",
    "        # sort the prob array and get the best top words index\n",
    "        X=[x for _,x in sorted(zip(prob,np.arange(self.V)),reverse=True)][0:self.top]\n",
    "        \n",
    "        # using the above top index in X we get the top words and return them\n",
    "        Preds=[self.Vocab[i] for i in X]\n",
    "        return Preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to predict the log-probability of next word using ngram\n",
    "\n",
    "# bigram model\n",
    "bimodel=Model(bidist,unidist,2,Voc_size,Vocab)\n",
    "\n",
    "# trigram model\n",
    "trimodel=Model(tridist,bidist,3,Voc_size,Vocab)\n",
    "\n",
    "# fourgram model\n",
    "fourmodel=Model(fourdist,tridist,4,Voc_size,Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBWKo5_Fmnbg"
   },
   "outputs": [],
   "source": [
    "str1 = 'after that alice said the'\n",
    "str2 = 'alice felt so desperate that she was'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ext_nVn2mvZt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for ' after that alice said the ' we have \n",
      "\n",
      "By Bigram model next 5 possible words are :\n",
      "['queen', 'king', 'hatter', 'caterpillar', 'white']\n",
      "Hence the next 2 possible word sequences are\n",
      "\n",
      "'after that alice said the' *queen*\n",
      "'after that alice said the' *king*\n",
      "\n",
      "\n",
      "By Trigram model next 5 possible words are :\n",
      "['king', 'queen', 'hatter', 'caterpillar', 'mock']\n",
      "Hence the next 2 possible word sequences are\n",
      "\n",
      "'after that alice said the' *king*\n",
      "'after that alice said the' *queen*\n",
      "\n",
      "\n",
      "By Fourgram model next 5 possible words are :\n",
      "['looking', 'argue', 'put', 'partners—', 'unable']\n",
      "Hence the next 2 possible word sequences are\n",
      "\n",
      "'after that alice said the' *looking*\n",
      "'after that alice said the' *argue*\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for str1\n",
    "\n",
    "print(\"for \\' {} \\' we have\".format(str1),'\\n')\n",
    "p2=bimodel.predict(tuple(word_tokenize(str1)[-1:]))\n",
    "print(\"By Bigram model next 5 possible words are :\",p2,sep='\\n')\n",
    "print(\"Hence the next 2 possible word sequences are\\n\",\n",
    "      \"'{}' *{}*\".format(str1,p2[0]),\n",
    "      \"'{}' *{}*\".format(str1,p2[1]),'\\n',sep='\\n')\n",
    "\n",
    "p3=trimodel.predict(tuple(word_tokenize(str1)[-2:]))\n",
    "print(\"By Trigram model next 5 possible words are :\",p3,sep='\\n')\n",
    "print(\"Hence the next 2 possible word sequences are\\n\",\n",
    "      \"'{}' *{}*\".format(str1,p3[0]),\n",
    "      \"'{}' *{}*\".format(str1,p3[1]),'\\n',sep='\\n')\n",
    "\n",
    "p4=fourmodel.predict(tuple(word_tokenize(str1)[-3:]))\n",
    "print(\"By Fourgram model next 5 possible words are :\",p4,sep='\\n')\n",
    "print(\"Hence the next 2 possible word sequences are\\n\",\n",
    "      \"'{}' *{}*\".format(str1,p4[0]),\n",
    "      \"'{}' *{}*\".format(str1,p4[1]),'\\n',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for ' alice felt so desperate that she was ' we have \n",
      "\n",
      "By Bigram model next 5 possible words are :\n",
      "['going', 'quite', 'looking', 'sitting', 'good']\n",
      "Hence the next 2 possible word sequences are\n",
      "\n",
      "'alice felt so desperate that she was' *going*\n",
      "'alice felt so desperate that she was' *quite*\n",
      "\n",
      "\n",
      "By Trigram model next 5 possible words are :\n",
      "['looking', 'saying', 'close', 'near', 'considering']\n",
      "Hence the next 2 possible word sequences are\n",
      "\n",
      "'alice felt so desperate that she was' *looking*\n",
      "'alice felt so desperate that she was' *saying*\n",
      "\n",
      "\n",
      "By Fourgram model next 5 possible words are :\n",
      "['losing', 'walking', 'ready', 'dozing', 'quite']\n",
      "Hence the next 2 possible word sequences are\n",
      "\n",
      "'alice felt so desperate that she was' *losing*\n",
      "'alice felt so desperate that she was' *walking*\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for str2\n",
    "\n",
    "print(\"for \\' {} \\' we have\".format(str2),'\\n')\n",
    "p2=bimodel.predict(tuple(word_tokenize(str2)[-1:]))\n",
    "print(\"By Bigram model next 5 possible words are :\",p2,sep='\\n')\n",
    "print(\"Hence the next 2 possible word sequences are\\n\",\n",
    "      \"'{}' *{}*\".format(str2,p2[0]),\n",
    "      \"'{}' *{}*\".format(str2,p2[1]),'\\n',sep='\\n')\n",
    "\n",
    "p3=trimodel.predict(tuple(word_tokenize(str2)[-2:]))\n",
    "print(\"By Trigram model next 5 possible words are :\",p3,sep='\\n')\n",
    "print(\"Hence the next 2 possible word sequences are\\n\",\n",
    "      \"'{}' *{}*\".format(str2,p3[0]),\n",
    "      \"'{}' *{}*\".format(str2,p3[1]),'\\n',sep='\\n')\n",
    "\n",
    "p4=fourmodel.predict(tuple(word_tokenize(str2)[-3:]))\n",
    "print(\"By Fourgram model next 5 possible words are :\",p4,sep='\\n')\n",
    "print(\"Hence the next 2 possible word sequences are\\n\",\n",
    "      \"'{}' *{}*\".format(str2,p4[0]),\n",
    "      \"'{}' *{}*\".format(str2,p4[1]),'\\n',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are just experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she said to alice and said the queen who cares for really have done i know what latitude or two looking at alice and said the queen who cares for really have done i know what latitude or two looking at alice and said the queen who cares for really have done i\n"
     ]
    }
   ],
   "source": [
    "# just an interesting experiment trying to generate sentences\n",
    "# observe that it gets stuck on to some words\n",
    "# bigram is very simple model hence doesnt produce very diverse sentence and get stuck in a sequence of words\n",
    "\n",
    "c=0 \n",
    "pred=''\n",
    "strr='she said to'\n",
    "while pred != '.' and c < 50:\n",
    "    pred=bimodel.predict(tuple(word_tokenize(strr)[-1:]))\n",
    "    c+=1\n",
    "    strr=strr+' '+str(pred[0])\n",
    "print(strr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she said to herself suppose it doesn looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking\n"
     ]
    }
   ],
   "source": [
    "# observe that it gets stuck on to some words\n",
    "c=0 \n",
    "pred=''\n",
    "strr='she said to'\n",
    "while pred != '.' and c < 50:\n",
    "    pred=trimodel.predict(tuple(word_tokenize(strr)[-2:]))\n",
    "    c+=1\n",
    "    strr=strr+' '+str(pred[0])\n",
    "print(strr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she said to herself i wonder what you re talking about said alice looking down with wonder at the mouse s tail but why do looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking looking\n"
     ]
    }
   ],
   "source": [
    "# observe that it gets stuck on to some words\n",
    "# fourgram perfmakes the representation very sparse hence data size is not big enough\n",
    "\n",
    "c=0 \n",
    "pred=''\n",
    "strr='she said to'\n",
    "while pred != '.' and c < 50:\n",
    "    pred=fourmodel.predict(tuple(word_tokenize(strr)[-3:]))\n",
    "    c+=1\n",
    "    strr=strr+' '+str(pred[0])\n",
    "print(strr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We observe that as n increases from 2 to 4 generated sentence makes more sense"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Assignment_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
