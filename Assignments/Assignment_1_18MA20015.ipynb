{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fk0evCZ7U9WO"
   },
   "source": [
    "# **Assignment 1 on Natural Language Processing**\n",
    "\n",
    "### Date : 4th Sept, 2020\n",
    "\n",
    "#### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Il_b_LFKXi8t"
   },
   "source": [
    " # NLTK Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ss5CZjC2Xt0i"
   },
   "source": [
    "The [NLTK](https://www.nltk.org/) Python framework is generally used as an education and research tool. Tokenization, Stemming, Lemmatization, Punctuation, Character count, word count are some of these packages which will be discussed in this tutorial.\n",
    "\n",
    "**Installing Nltk** <br>\n",
    "Nltk can be installed using PIP or Conda package managers.For detailed installation instructions follow this [link](https://www.nltk.org/install.html).\n",
    "\n",
    "To ensure we are all on the same page, the coding environment will be in **python3**. We suggest downloading Anaconda3 and creating a separate environment to do this assignment. \n",
    "The link to anaconda3 for Windows and Linux is available here https://docs.anaconda.com/anaconda/install/. \n",
    "The steps to install NLTK is available on the link: \n",
    "```bash\n",
    "sudo pip3 install nltk \n",
    "python3 \n",
    "nltk.download()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r4txbU5-RlMv"
   },
   "source": [
    "**Note for Question and answers:**\n",
    "\n",
    "Write your answers to the point in the text box below labelled as **Answer here**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "52_aJRSqaHgC"
   },
   "source": [
    "# Tokenizing words and Sentences using Nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o5_ElYaeaMbR"
   },
   "source": [
    "**Tokenization** is the process by which big quantity of text is divided into smaller parts called tokens. <br>It is crucial to understand the pattern in the text in order to perform various NLP tasks.These tokens are very useful for finding such patterns.<br>\n",
    "\n",
    "Natural Language toolkit has very important module tokenize which further comprises of sub-modules\n",
    "\n",
    "1. word tokenize\n",
    "2. sentence tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sby_OS3qZ_fz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\harshal\n",
      "[nltk_data]     d\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to C:\\Users\\harshal\n",
      "[nltk_data]     d\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing modules\n",
    "import nltk\n",
    "nltk.download('punkt') # For tokenizers\n",
    "nltk.download('inaugural') # For dataset\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ew9Aq5WHXSn-"
   },
   "outputs": [],
   "source": [
    "# Sample corpus.\n",
    "from nltk.corpus import inaugural\n",
    "corpus = inaugural.raw('1789-Washington.txt')\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2wbXKzVW0GO"
   },
   "source": [
    "### **TASK**:\n",
    "\n",
    "For the given corpus, \n",
    "1. Print the number of sentences and tokens. \n",
    "2. Print the average number of tokens per sentence.\n",
    "3. Print the number of unique tokens\n",
    "4. Print the number of tokens after stopword removal using the stopwords from nltk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jrtu9HcHXFe6"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "corpus_sents=sent_tokenize(corpus)\n",
    "corpus_toks=word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens in corpus is 1537\n",
      "number of sentences in corpus is 23\n"
     ]
    }
   ],
   "source": [
    "# 1.Printing the number of sentences and tokens.\n",
    "\n",
    "num_toks=len(corpus_toks)\n",
    "num_sents=len(corpus_sents)\n",
    "\n",
    "print(\"number of tokens in corpus is {}\".format(num_toks)\n",
    "      ,\"number of sentences in corpus is {}\".format(num_sents),sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per sentence is 66.82608695652173\n"
     ]
    }
   ],
   "source": [
    "# 2. Printing the average number of tokens per sentence\n",
    "# avg_tok_per_sent = total number of tokens/total number of sentences\n",
    "\n",
    "avg_tok_per_sent = num_toks/num_sents\n",
    "\n",
    "print(\"Average number of tokens per sentence is {}\".format(avg_tok_per_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of  unique tokens in the corpus is 626\n"
     ]
    }
   ],
   "source": [
    "# 3. Printing the number of unique tokens\n",
    "\n",
    "print(\"Number of  unique tokens in the corpus is {}\".format( len(set(corpus_toks)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\harshal\n",
      "[nltk_data]     d\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing the requirements\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens after removing stopwords are 800\n"
     ]
    }
   ],
   "source": [
    "# 4. Printing the number of tokens after stopword removal \n",
    "\n",
    "# list of all the eglish stopwords\n",
    "eng_stopwords=stopwords.words('english')\n",
    "\n",
    "# note same token might be counted twice as it was just mentioned tokens and not unique tokens\n",
    "# for unique tokens we can just take the set difference\n",
    "\n",
    "# going through all the tokens and adding it to the filtered list if it is not in the stopwords list\n",
    "\n",
    "num_filtered_toks = [i for i in corpus_toks if i not in eng_stopwords]\n",
    "print(\"Number of tokens after removing stopwords are {}\".format(len(num_filtered_toks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UViYY9_3t2UE"
   },
   "source": [
    "# Stemming and Lemmatization with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g55XX9KDLgO7"
   },
   "source": [
    "**What is Stemming?** <br>\n",
    "Stemming is a kind of normalization for words. Normalization is a technique where a set of words in a sentence are converted into a sequence to shorten its lookup. The words which have the same meaning but have some variation according to the context or sentence are normalized.<br>\n",
    "Hence Stemming is a way to find the root word from any variations of respective word\n",
    "\n",
    "There are many stemmers provided by Nltk like **PorterStemmer**, **SnowballStemmer**, **LancasterStemmer**.<br>\n",
    "\n",
    "We will try and see differences between Porterstemmer and Snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SS4Ij__XLfTB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words\n",
      " ['grows', 'leaves', 'fairly', 'cats', 'trouble', 'misunderstanding', 'friendships', 'easily', 'rational', 'relational'] \n",
      "\n",
      "Stemmed words using SnowballStemmer\n",
      " ['grow', 'leav', 'fair', 'cat', 'troubl', 'misunderstand', 'friendship', 'easili', 'ration', 'relat'] \n",
      "\n",
      "Stemmed words using PorterStemmer\n",
      " ['grow', 'leav', 'fairli', 'cat', 'troubl', 'misunderstand', 'friendship', 'easili', 'ration', 'relat'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer # Note that SnowballStemmer has language as parameter.\n",
    "\n",
    "words = [\"grows\",\"leaves\",\"fairly\",\"cats\",\"trouble\",\"misunderstanding\",\"friendships\",\"easily\", \"rational\", \"relational\"]\n",
    "\n",
    "# TODO\n",
    "# create an instance of both the stemmers and perform stemming on above words\n",
    "SS_stemmer = SnowballStemmer(\"english\")\n",
    "PS_stemmer = PorterStemmer()\n",
    "\n",
    "# for each word in the list get its stem\n",
    "stemmed_words_SS=[SS_stemmer.stem(w) for w in words]\n",
    "stemmed_words_PS=[PS_stemmer.stem(w) for w in words]\n",
    "\n",
    "print(\"Original words\\n\",words,'\\n')\n",
    "print(\"Stemmed words using SnowballStemmer\\n\",stemmed_words_SS,'\\n')\n",
    "print(\"Stemmed words using PorterStemmer\\n\",stemmed_words_PS,'\\n')\n",
    "\n",
    "# TODO\n",
    "# Complete the function which takes a sentence/corpus and gets its stemmed version.\n",
    "def stemSentence(sentence=None):\n",
    "    if sentence == None:\n",
    "        return \n",
    "\n",
    "    # for stemming the words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    # first get the words\n",
    "    words=word_tokenize(sentence)\n",
    "    \n",
    "    # getting the stemmed words\n",
    "    stems=[stemmer.stem(t) for t in words]\n",
    "    \n",
    "    # joining them to form one string again\n",
    "    sentence_stem = ' '.join(stems)\n",
    "    \n",
    "    return sentence_stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hie there word world'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=\"hie there wordly world\"\n",
    "\n",
    "stemSentence(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0JuE8CuDQSno"
   },
   "source": [
    "**What is Lemmatization?** <br>\n",
    "Lemmatization is the algorithmic process of finding the lemma of a word depending on their meaning. Lemmatization usually refers to the morphological analysis of words, which aims to remove inflectional endings. It helps in returning the base or dictionary form of a word, which is known as the lemma.<br>\n",
    "\n",
    "*The NLTK Lemmatization method is based on WorldNet's built-in morph function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "noyl1YNsQp98"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\harshal\n",
      "[nltk_data]     d\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas using default pos\n",
      " ['grows', 'leaf', 'fairly', 'cat', 'trouble', 'running', 'friendship', 'easily', 'wa', 'relational', 'ha'] \n",
      "\n",
      "Lemmas using pos='n'\n",
      " ['grows', 'leaf', 'fairly', 'cat', 'trouble', 'running', 'friendship', 'easily', 'wa', 'relational', 'ha'] \n",
      "\n",
      "Lemmas using pos='v'\n",
      " ['grow', 'leave', 'fairly', 'cat', 'trouble', 'run', 'friendships', 'easily', 'be', 'relational', 'have'] \n",
      "\n",
      "Lemmas using pos='a'\n",
      " ['grows', 'leaves', 'fairly', 'cats', 'trouble', 'running', 'friendships', 'easily', 'was', 'relational', 'has'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet') # Since Lemmatization method is based on WorldNet's built-in morph function.\n",
    "\n",
    "words = [\"grows\",\"leaves\",\"fairly\",\"cats\",\"trouble\",\"running\",\"friendships\",\"easily\", \"was\", \"relational\",\"has\"]\n",
    "\n",
    "#TODO\n",
    "# Create an instance of the Lemmatizer and perform Lemmatization on above words\n",
    "# You can also give Parts-of-speech(pos) to the Lemmatizer for example \"v\" (verb). Check the differences in the outputs.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "lemmas = [ lemmatizer.lemmatize(w) for w in words ]\n",
    "lemmas_v = [ lemmatizer.lemmatize(w,pos='v') for w in words ]\n",
    "lemmas_n = [ lemmatizer.lemmatize(w,pos='n') for w in words ]\n",
    "lemmas_a = [ lemmatizer.lemmatize(w,pos='a') for w in words ]\n",
    "print(\"Lemmas using default pos\\n\",lemmas,'\\n')\n",
    "print(\"Lemmas using pos='n'\\n\",lemmas_n,'\\n')\n",
    "print(\"Lemmas using pos='v'\\n\",lemmas_v,'\\n')\n",
    "print(\"Lemmas using pos='a'\\n\",lemmas_a,'\\n')\n",
    "\n",
    "\n",
    "#TODO\n",
    "# Complete the function which takes a sentence/corpus and gets its lemmatized version.\n",
    "def lemmatizeSentence(sentence=None,pos='n'):\n",
    "    if sentence == None:\n",
    "        return \n",
    "\n",
    "    # instance of the Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "    # first get the words\n",
    "    words=word_tokenize(sentence)\n",
    "\n",
    "    # getting the lemmatized words using default i.e noun form\n",
    "    lemma=[lemmatizer.lemmatize(t,pos=pos) for t in words]\n",
    "\n",
    "    # joining them to form one string again\n",
    "    sentence_lemma = ' '.join(lemma)\n",
    "\n",
    "    return sentence_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hie there wordly world grows leaf'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=\"hie there wordly world grows leaves\"\n",
    "\n",
    "lemmatizeSentence(s,pos='n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJW6HsycSAlU"
   },
   "source": [
    "**Question:** Give example of two words which have same stem but different lemma? Show the stem and lemma of both words in the code below \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zcq6bUEaSAt1"
   },
   "source": [
    "**Answer here:** \n",
    "1. Natural \n",
    "2. Nature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8OtIEmFkGBM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem of Natural is natur\n",
      "lemma of Natural is Natural\n",
      "\n",
      "\n",
      "stem of Nature is natur\n",
      "lemma of Nature is Nature\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "# Write code to print the stem and lemma of both your words\n",
    "s1,s2='Natural','Nature'\n",
    "\n",
    "s_stem=stemSentence(s1)\n",
    "s_lemma=lemmatizeSentence(s1,pos='a')\n",
    "print(\"stem of {} is {}\\nlemma of {} is {}\".format(s1,s_stem,s1,s_lemma))\n",
    "print('\\n')\n",
    "\n",
    "s_stem=stemSentence(s2)\n",
    "s_lemma=lemmatizeSentence(s2,pos='a')\n",
    "print(\"stem of {} is {}\\nlemma of {} is {}\".format(s2,s_stem,s2,s_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0tz3SIGSA2b"
   },
   "source": [
    "**Question:** Write a comparison between stemming and lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aks8QaQ_SA_N"
   },
   "source": [
    "**Answer here:**\n",
    "\n",
    "| Stemming|Lemmatization|\n",
    "|:-----|:----|\n",
    "|1. Rule based method to reduce words to their stem form e.g. removing the suffix 'ing' etc | 1. Dictionary based method to reduce words to their root form e.g. WordNetLemmatizer uses wordnet for this task and finds the lemma of the words|\n",
    "|2. As it is rule based it is simple and fast method | 2. As it is dictionary based it is relatively slow method |\n",
    "|3. Since all rules dont apply in general it sometimes gives an output which is not a valid word |3. Since it uses dictionary it works quite well and output is always a valid word in the language |\n",
    "|4. It doesnt require part of speech tag to work | 4. It requires part of speech tag to work, if tag is not specified tag 'Noun' is assumed|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Assignment 1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
