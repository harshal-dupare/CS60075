{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyJ25uz0kSaw"
   },
   "source": [
    "# Assignment 3 on Natural Language Processing\n",
    "\n",
    "## Date : 30th Sept, 2020\n",
    "\n",
    "### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ao1nhg9RknmF"
   },
   "source": [
    "The central idea of this assignment is to use Naive Bayes classifier and LSTM based classifier and compare the models by accuracy on IMDB dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONM5Q4SCe9Mr"
   },
   "source": [
    "Please submit with outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ElRkQElWUMjG"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fhHRim2AUm4z"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the IMDB dataset. You can load it using pandas as dataframe\n",
    "df=pd.read_csv('IMDB Dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK_Hn2f6VMP7"
   },
   "source": [
    "# Preprocessing\n",
    "PrePrecessing that needs to be done on lower cased corpus\n",
    "\n",
    "1. Remove html tags\n",
    "2. Remove URLS\n",
    "3. Remove non alphanumeric character\n",
    "4. Remove Stopwords\n",
    "5. Perform stemming and lemmatization\n",
    "\n",
    "You can use regex from re. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing re expressions to remove \n",
    "res = r'<.*?>|http\\S+|www\\S+|[^a-zA-Z\\s]|\\n' # regular expression for preprocessing the input\n",
    "reprocess = re.compile(res)\n",
    "df['review'] = df['review'].apply(lambda x: re.sub(reprocess, '', x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords and applying stemming and lemmaitization\n",
    "# takes 1-2 mins\n",
    "df['review'] = df['review'].apply(lambda x: snowball_stemmer.stem(' '.join([w for w in word_tokenize(x) if w not in eng_stopwords])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one reviewers mentioned watching oz episode youll hooked right exactly happened methe first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use wordit called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home manyaryans muslims gangstas latinos christians italians irish moreso scuffles death stares dodgy dealings shady agreements never far awayi would say main appeal show due fact goes shows wouldnt dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz doesnt mess around first episode ever saw struck nasty surreal couldnt say ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards wholl sold nickel inmates wholl kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewingthats get touch darker sid'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final preprocessed data 'review' column\n",
    "df['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DyaSkfcvYGXk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    25000\n",
       "positive    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print Statistics of Data like avg length of sentence , proposition of data w.r.t class labels\n",
    "grp = df.groupby(['sentiment'])\n",
    "pos_review_length = (grp.get_group('positive'))['review'].apply(lambda x: len(x.split(' ')))\n",
    "neg_review_length = (grp.get_group('negative'))['review'].apply(lambda x: len(x.split(' ')))\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive reviews\n",
      " mean: 119.92256\n",
      " standard deviation: 92.96069271827898\n",
      "\n",
      "Negative reviews\n",
      " mean: 117.22948\n",
      " standard deviation: 85.34994530435395\n"
     ]
    }
   ],
   "source": [
    "print('Positive reviews\\n mean: {}\\n standard deviation: {}\\n'.format(pos_review_length.mean(),pos_review_length.var()**0.5))\n",
    "print('Negative reviews\\n mean: {}\\n standard deviation: {}'.format(neg_review_length.mean(),neg_review_length.var()**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearly identical distribution\n"
     ]
    }
   ],
   "source": [
    "plt.hist(pos_review_length,bins=100,density=1,histtype='step')\n",
    "plt.title('distribution of postive review length')\n",
    "plt.hist(neg_review_length,bins=100,density=1,histtype='step')\n",
    "plt.title('distribution of review lengths')\n",
    "plt.legend(['positive','negative'])\n",
    "plt.show()\n",
    "print(\"Nearly identical distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FkJ-e2pUwun"
   },
   "source": [
    "# Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eVq-mN28U_J4"
   },
   "outputs": [],
   "source": [
    "# get reviews column from df\n",
    "reviews = df['review']\n",
    "\n",
    "# get labels column from df\n",
    "labels =df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ljo5NquhXTXr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'positive']\n"
     ]
    }
   ],
   "source": [
    "# Use label encoder to encode labels. Convert to 0/1\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "\n",
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wzG-C_EVWWET"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test (80% - 20%). \n",
    "# Use stratify in train_test_split so that both train and test have similar ratio of positive and negative samples.\n",
    "\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, \n",
    "                                                                               encoded_labels, \n",
    "                                                                               test_size=0.20, \n",
    "                                                                               random_state=3141, \n",
    "                                                                               stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz1YdsSkiWCX"
   },
   "source": [
    "Here there are two approaches possible for building vocabulary for the naive Bayes.\n",
    "1. Take the whole data (train + test) to build the vocab. In this way while testing there is no word which will be out of vocabulary.\n",
    "2. Take the train data to build vocab. In this case, some words from the test set may not be in vocab and hence one needs to perform smoothing so that one the probability term is not zero.\n",
    " \n",
    "You are supposed to go by the 2nd approach.\n",
    " \n",
    "Also building vocab by taking all words in the train set is memory intensive, hence you are required to build vocab by choosing the top 2000 - 3000 frequent words in the training corpus.\n",
    "\n",
    "> $ P(x_i | w_j) = \\frac{ N_{x_i,w_j}\\, +\\, \\alpha }{ N_{w_j}\\, +\\, \\alpha*d} $\n",
    "\n",
    "\n",
    "$N_{x_i,w_j}$ : Number of times feature $x_i$ appears in samples of class $w_j$\n",
    "\n",
    "$N_{w_j}$ : Total count of features in class $w_j$\n",
    "\n",
    "$\\alpha$ : Parameter for additive smoothing. Here consider $\\alpha$ = 1\n",
    "\n",
    "$d$ : Dimentionality of the feature vector  $x = [x_1,x_2,...,x_d]$. In our case its the vocab size.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_Bayes(object):\n",
    "    \"\"\"\n",
    "    Class to store the Naive Bayes Model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_sent, train_lab, alpha=1, vocab_size=3000):\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.train_sent = train_sent.reset_index(drop=True)\n",
    "        self.train_lab = train_lab \n",
    "\n",
    "        self.pos_sent = self.train_sent[np.argwhere(self.train_lab).squeeze().tolist()]\n",
    "        self.neg_sent = self.train_sent.drop(np.argwhere(self.train_lab).squeeze().tolist())\n",
    "\n",
    "        # class probabilites\n",
    "        self.P_pos = len(self.pos_sent)/len(self.train_lab)\n",
    "        self.P_neg = len(self.neg_sent)/len(self.train_lab)\n",
    "\n",
    "        # training data to build vocab and only take top 'vocab_size' number of words\n",
    "        self.vectorizer = CountVectorizer(max_features = self.vocab_size)\n",
    "        self.vec = self.vectorizer.fit_transform(self.train_sent) \n",
    "        self.vocab = self.vectorizer.get_feature_names()\n",
    "        \n",
    "        # count of words in positive and negative classes by sentences/reviews\n",
    "        self.vec_pos = self.vectorizer.transform(self.pos_sent)\n",
    "        self.vec_neg = self.vectorizer.transform(self.neg_sent)\n",
    "        self.total_pos = self.vec_pos.sum()\n",
    "        self.total_neg = self.vec_neg.sum()\n",
    "        \n",
    "        # calculating the log probs of all the words in vocab\n",
    "        self.word_log_prob_dict = {0:{},1:{}}\n",
    "        self.__set_word_log_prob__()\n",
    "        \n",
    "    # returns log probability of word given sentiment class\n",
    "    # Using laplace smoothing for words in test set not present in vocab of train set\n",
    "    def __get_word_log_prob__(self, word, sentiment):  \n",
    "        if sentiment == 1:\n",
    "            count_word = self.vec_pos[:, self.vocab.index(word)].sum() if word in self.vocab else 0\n",
    "\n",
    "            return np.log((count_word + self.alpha)/(self.total_pos + self.alpha*self.vocab_size))  \n",
    "        elif sentiment == 0:\n",
    "            count_word = self.vec_neg[:, self.vocab.index(word)].sum() if word in self.vocab else 0 \n",
    "\n",
    "            return np.log((count_word + self.alpha)/(self.total_pos + self.alpha*self.vocab_size))\n",
    "        \n",
    "    def __set_word_log_prob__(self):\n",
    "        for word in self.vocab :\n",
    "            self.word_log_prob_dict[0][word]=self.__get_word_log_prob__(word,0)\n",
    "            self.word_log_prob_dict[1][word]=self.__get_word_log_prob__(word,1)\n",
    "    \n",
    "    def __predict_single__(self, sentence): \n",
    "        log_prob_pos = np.log(self.P_pos)\n",
    "        log_prob_neg = np.log(self.P_neg)\n",
    "\n",
    "        for word in word_tokenize(sentence):\n",
    "            ## if word is not found in the vocab then continue\n",
    "            if word not in self.vocab:\n",
    "                continue\n",
    "            log_prob_pos += self.word_log_prob_dict[1][word]\n",
    "            log_prob_neg += self.word_log_prob_dict[0][word]\n",
    "\n",
    "        return 1 if log_prob_pos > log_prob_neg else 0\n",
    "    \n",
    "    def predict(self,sentences):\n",
    "        sentences = list(sentences)\n",
    "        predictions = []\n",
    "        for sentence in sentences:\n",
    "            predictions.append(self.__predict_single__(sentence))\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "\n",
    "NBclassifier = Naive_Bayes(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on testing data\n",
    "\n",
    "test_predictions = NBclassifier.predict(test_sentences.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84      5000\n",
      "           1       0.86      0.82      0.84      5000\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlNql0acU7sa"
   },
   "source": [
    "# *LSTM* based Classifier\n",
    "\n",
    "Use the above train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SkqnvbUOXoN0"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters of the model\n",
    "vocab_size = 3000 # choose based on statistics\n",
    "oov_tok = '<OOK>'\n",
    "embedding_dim = 100\n",
    "max_length = 300 # choose based on statistics, for example 150 to 200\n",
    "padding_type='post'\n",
    "trunc_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UeycEg9nZAOF"
   },
   "outputs": [],
   "source": [
    "# tokenize sentences\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# convert Test dataset to sequence and pad sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Mtw3w895ZP39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 100)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                3096      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 387,601\n",
      "Trainable params: 387,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model initialization\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
    "    keras.layers.Dense(24, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "skmaDJMnZTzc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1125/1125 [==============================] - 123s 109ms/step - loss: 0.4320 - accuracy: 0.8037 - val_loss: 0.3607 - val_accuracy: 0.8493\n",
      "Epoch 2/5\n",
      "1125/1125 [==============================] - 126s 112ms/step - loss: 0.3162 - accuracy: 0.8739 - val_loss: 0.3223 - val_accuracy: 0.8640\n",
      "Epoch 3/5\n",
      "1125/1125 [==============================] - 124s 110ms/step - loss: 0.2673 - accuracy: 0.8947 - val_loss: 0.3319 - val_accuracy: 0.8553\n",
      "Epoch 4/5\n",
      "1125/1125 [==============================] - 124s 111ms/step - loss: 0.2355 - accuracy: 0.9086 - val_loss: 0.3154 - val_accuracy: 0.8767\n",
      "Epoch 5/5\n",
      "1125/1125 [==============================] - 125s 111ms/step - loss: 0.2444 - accuracy: 0.9034 - val_loss: 0.3464 - val_accuracy: 0.8593\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "history = model.fit(train_padded, train_labels, \n",
    "                    epochs=num_epochs, verbose=1, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TjEhWEr5Zq7M"
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy on Test data\n",
    "\n",
    "# Get probabilities\n",
    "prediction_probs = model.predict(test_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.87      5000\n",
      "           1       0.92      0.81      0.86      5000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get labels based on probability 1 if p>= 0.5 else 0\n",
    "test_predicted = np.where(prediction_probs >= 0.5, 1, 0)\n",
    "\n",
    "# Accuracy : one can use classification_report from sklearn\n",
    "print(classification_report(test_labels, test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIICV-ySOYL0"
   },
   "source": [
    "## Get predictions for random examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "m2RmfNL3OYL0"
   },
   "outputs": [],
   "source": [
    "# reviews on which we need to predict\n",
    "sentence = [\"The movie was very touching and heart whelming\", \n",
    "            \"I have never seen a terrible movie like this\", \n",
    "            \"the movie plot is terrible but it had good acting\"]\n",
    "\n",
    "# convert to a sequence\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "\n",
    "# pad the sequence\n",
    "padded =  pad_sequences(sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# Get probabilities\n",
    "predicts = model.predict(padded)\n",
    "\n",
    "# Get labels based on probability 1 if p>= 0.5 else 0\n",
    "sentiments = np.where(predicts >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For `The movie was very touching and heart whelming` Predicted Sentiment is Positive\n",
      "\n",
      "For `I have never seen a terrible movie like this` Predicted Sentiment is Negative\n",
      "\n",
      "For `the movie plot is terrible but it had good acting` Predicted Sentiment is Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(sentence):\n",
    "    print('For','`'+sent+'`','Predicted Sentiment is {}\\n'.format('Positive' if sentiments[i][0] else 'Negative'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Assignment -3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
